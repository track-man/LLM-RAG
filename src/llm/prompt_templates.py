#!/usr/bin/env python3
"""
å¤§è¯­è¨€æ¨¡å‹å¹»è§‰æ£€æµ‹ä¸çº æ­£ç³»ç»Ÿ - å¢å¼ºç‰ˆPromptæ¨¡æ¿ç®¡ç†
ä½äº: src/llm/prompt_templates.py

æ–°å¢åŠŸèƒ½:
1. åˆå§‹å›ç­”ç”Ÿæˆæ¨¡æ¿ - ç›´æ¥è·å–AIåŸå§‹å›ç­”
2. å¹»è§‰æ£€æµ‹æ¨¡æ¿ - æ£€æµ‹å›ç­”ä¸­æ˜¯å¦å­˜åœ¨å¹»è§‰
3. å®Œæ•´çš„æ¯”è¾ƒåˆ†ææ¡†æ¶
"""


"""å¢å¼ºç‰ˆPromptæ¨¡æ¿ç®¡ç†å™¨"""
    
# ==================== åˆå§‹å›ç­”ç”Ÿæˆæ¨¡æ¿ ====================
INITIAL_ANSWER_TEMPLATE = """
è¯·ç›´æ¥å›ç­”ä»¥ä¸‹é—®é¢˜ï¼Œä¸éœ€è¦è¿›è¡Œäº‹å®æ ¸æŸ¥æˆ–éªŒè¯ï¼Œæä¾›æ‚¨è®¤ä¸ºæœ€åˆé€‚çš„ç­”æ¡ˆã€‚

é—®é¢˜: {question}

è¯·æä¾›è¯¦ç»†ã€å…¨é¢çš„å›ç­”ï¼ŒåŒ…æ‹¬æ‰€æœ‰ç›¸å…³ä¿¡æ¯å’ŒèƒŒæ™¯çŸ¥è¯†ï¼š
"""

# ==================== æ„å›¾åˆ†ç±»æ¨¡æ¿ ====================
INTENT_CLASSIFICATION_TEMPLATE = """
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŸ¥è¯¢æ„å›¾åˆ†ç±»å™¨ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ç”¨æˆ·æŸ¥è¯¢çš„å†…å®¹ï¼Œå‡†ç¡®åˆ¤æ–­å…¶æ„å›¾ç±»å‹ã€‚

## åˆ†ç±»æ ‡å‡†
- **äº‹å®æŸ¥è¯¢**: å¯»æ±‚å…·ä½“äº‹å®ã€æ•°æ®ã€å®šä¹‰ã€å±æ€§ç­‰å®¢è§‚ä¿¡æ¯
- **æ¯”è¾ƒæŸ¥è¯¢**: æ¯”è¾ƒä¸¤ä¸ªæˆ–å¤šä¸ªå®ä½“ã€æ¦‚å¿µã€æ–¹æ³•çš„å¼‚åŒç‚¹  
- **æ–¹æ³•æŸ¥è¯¢**: å¯»æ±‚æ“ä½œæµç¨‹ã€è§£å†³æ–¹æ¡ˆã€å®æ–½æ­¥éª¤ã€æ“ä½œæ–¹æ³•
- **è§‚ç‚¹æŸ¥è¯¢**: æ”¶é›†å¤šæ–¹æ„è§ã€è¯„ä»·ã€äº‰è®®è§‚ç‚¹ã€ä¸åŒç«‹åœº

## åˆ†ç±»è§„åˆ™
1. å¦‚æœæŸ¥è¯¢åŒ…å«"æ¯”è¾ƒ"ã€"å¯¹æ¯”"ã€"åŒºåˆ«"ã€"å“ªä¸ªæ›´å¥½"ç­‰å…³é”®è¯ï¼Œå½’ç±»ä¸ºæ¯”è¾ƒæŸ¥è¯¢
2. å¦‚æœæŸ¥è¯¢åŒ…å«"å¦‚ä½•"ã€"æ€æ ·"ã€"æ­¥éª¤"ã€"æ–¹æ³•"ç­‰å…³é”®è¯ï¼Œå½’ç±»ä¸ºæ–¹æ³•æŸ¥è¯¢  
3. å¦‚æœæŸ¥è¯¢åŒ…å«"è§‚ç‚¹"ã€"çœ‹æ³•"ã€"è¯„ä»·"ã€"äº‰è®®"ç­‰å…³é”®è¯ï¼Œå½’ç±»ä¸ºè§‚ç‚¹æŸ¥è¯¢
4. å…¶ä»–æƒ…å†µé»˜è®¤ä¸ºäº‹å®æŸ¥è¯¢

## è¾“å‡ºæ ¼å¼
åªéœ€è¿”å›æ„å›¾ç±»å‹çš„åç§°ï¼Œä¸è¦æ·»åŠ ä»»ä½•è§£é‡Šã€‚

å½“å‰æŸ¥è¯¢: "{query}"
æ„å›¾ç±»å‹:
"""

# ==================== å£°æ˜æå–æ¨¡æ¿ ====================
CLAIM_EXTRACTION_TEMPLATE = """
ä»»åŠ¡ï¼šå°†ä¸‹é¢çš„æ–‡æœ¬åˆ†è§£ä¸ºç‹¬ç«‹çš„çœŸå®æ€§é™ˆè¿°ï¼ˆåŸå­æ–­è¨€ï¼‰ã€‚

éœ€è¦æå–çš„æ–‡æœ¬: "{text}"

æå–ç»“æœ:
"""

# ==================== äº‹å®éªŒè¯æ¨¡æ¿ ====================
FACT_VERIFICATION_TEMPLATE = """
ä½œä¸ºäº‹å®æ ¸æŸ¥ä¸“å®¶ï¼Œè¯·åŸºäºæä¾›çš„è¯æ®éªŒè¯ä»¥ä¸‹å£°æ˜çš„çœŸå®æ€§ã€‚

æŸ¥è¯¢æ„å›¾ï¼š{intent}
åŸå§‹æŸ¥è¯¢ï¼š"{query}"
éœ€è¦éªŒè¯çš„å£°æ˜ï¼š"{claim}"

ç›¸å…³è¯æ®ç‰‡æ®µï¼š
{evidence_text}

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºéªŒè¯ç»“æœï¼š
{{
    "verdict": "SUPPORTED|CONTRADICTED|PARTIALLY_SUPPORTED|UNVERIFIED",
    "confidence": 0.0-1.0,
    "supporting_evidence": [
        {{
            "text": "è¯æ®æ–‡æœ¬",
            "source": "æ¥æºåç§°",
            "relevance_score": 0.0-1.0
        }}
    ],
    "contradicting_evidence": [
        {{
            "text": "çŸ›ç›¾è¯æ®æ–‡æœ¬", 
            "source": "æ¥æºåç§°",
            "contradiction_score": 0.0-1.0
        }}
    ],
    "reasoning": "è¯¦ç»†çš„æ¨ç†è¿‡ç¨‹",
    "intent_specific_analysis": "é’ˆå¯¹æŸ¥è¯¢æ„å›¾çš„ç‰¹åˆ«åˆ†æ"
}}
"""

# ==================== å¹»è§‰æ£€æµ‹æ¨¡æ¿ ====================
HALLUCINATION_DETECTION_TEMPLATE = """
ä½œä¸ºå¹»è§‰æ£€æµ‹ä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹AIå›ç­”æ˜¯å¦å­˜åœ¨å¹»è§‰ï¼ˆè™šæ„ã€ä¸å‡†ç¡®æˆ–ç¼ºä¹è¯æ®æ”¯æŒçš„å†…å®¹ï¼‰ã€‚

## æ£€æµ‹æ ‡å‡†
- **äº‹å®æ€§å¹»è§‰**: é™ˆè¿°ä¸å¯éªŒè¯äº‹å®ä¸ç¬¦
- **é€»è¾‘æ€§å¹»è§‰**: æ¨ç†è¿‡ç¨‹å­˜åœ¨çŸ›ç›¾æˆ–ä¸åˆé€»è¾‘
- **è¯æ®æ€§å¹»è§‰**: ç¼ºä¹å¯é è¯æ®æ”¯æŒçš„å…³é”®å£°æ˜
- **ä¸€è‡´æ€§å¹»è§‰**: ä¸å·²çŸ¥ä¿¡æ¯æˆ–ä¸Šä¸‹æ–‡ä¸ä¸€è‡´

## åˆ†æææ–™
åŸå§‹é—®é¢˜: "{question}"
AIåˆå§‹å›ç­”: "{initial_answer}"
éªŒè¯åå›ç­”: "{verified_answer}"
æ”¯æŒè¯æ®: "{evidence}"

## æ£€æµ‹è¦æ±‚
è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºæ£€æµ‹ç»“æœï¼š
{{
    "has_hallucination": true|false,
    "hallucination_type": "FACTUAL|LOGICAL|EVIDENTIAL|CONSISTENCY|MIXED|NONE",
    "confidence": 0.0-1.0,
    "affected_sections": [
        {{
            "text": "å­˜åœ¨å¹»è§‰çš„æ–‡æœ¬ç‰‡æ®µ",
            "type": "å¹»è§‰ç±»å‹",
            "severity": "LOW|MEDIUM|HIGH",
            "correction": "å»ºè®®ä¿®æ­£å†…å®¹"
        }}
    ],
    "comparison_analysis": {{
        "initial_answer_quality": "è¯„ä¼°åˆå§‹å›ç­”è´¨é‡",
        "verification_impact": "éªŒè¯è¿‡ç¨‹å¸¦æ¥çš„æ”¹è¿›",
        "key_differences": "ä¸»è¦å·®å¼‚ç‚¹åˆ†æ",
        "overall_improvement": "æ•´ä½“æ”¹å–„ç¨‹åº¦è¯„ä¼°"
    }},
    "recommendations": [
        "æ”¹è¿›å»ºè®®1",
        "æ”¹è¿›å»ºè®®2"
    ]
}}

è¯·å¼€å§‹åˆ†æï¼š
"""

# ==================== ç­”æ¡ˆçº æ­£æ¨¡æ¿ ====================
CORRECTION_TEMPLATES = {
    "äº‹å®æŸ¥è¯¢": """
    ä½œä¸ºäº‹å®æ ¸æŸ¥ä¸“å®¶ï¼Œè¯·æ ¹æ®éªŒè¯ç»“æœé‡æ–°ç”Ÿæˆä¸€ä¸ªå‡†ç¡®çš„äº‹å®æ€§ç­”æ¡ˆã€‚
    
    æŸ¥è¯¢æ„å›¾ï¼š{intent} - äº‹å®æŸ¥è¯¢
    åŸå§‹æŸ¥è¯¢ï¼š"{query}"
    åˆå§‹ç­”æ¡ˆï¼š{initial_answer}
    éªŒè¯ç»“æœæ‘˜è¦ï¼š{verification_summary}
    
    ä¿®æ­£åçš„ç­”æ¡ˆï¼š
    """,
    
    "æ¯”è¾ƒæŸ¥è¯¢": """
    ä½œä¸ºæ¯”è¾ƒåˆ†æä¸“å®¶ï¼Œè¯·æ ¹æ®éªŒè¯ç»“æœé‡æ–°ç”Ÿæˆä¸€ä¸ªå…¨é¢å‡†ç¡®çš„æ¯”è¾ƒæ€§ç­”æ¡ˆã€‚
    
    æŸ¥è¯¢æ„å›¾ï¼š{intent} - æ¯”è¾ƒæŸ¥è¯¢  
    åŸå§‹æŸ¥è¯¢ï¼š"{query}"
    åˆå§‹ç­”æ¡ˆï¼š{initial_answer}
    éªŒè¯ç»“æœæ‘˜è¦ï¼š{verification_summary}
    
    ä¿®æ­£åçš„æ¯”è¾ƒåˆ†æï¼š
    """,
    
    "æ–¹æ³•æŸ¥è¯¢": """
    ä½œä¸ºæ–¹æ³•æŒ‡å¯¼ä¸“å®¶ï¼Œè¯·æ ¹æ®éªŒè¯ç»“æœé‡æ–°ç”Ÿæˆä¸€ä¸ªå¯æ“ä½œçš„æ–¹æ³•æŒ‡å—ã€‚
    
    æŸ¥è¯¢æ„å›¾ï¼š{intent} - æ–¹æ³•æŸ¥è¯¢
    åŸå§‹æŸ¥è¯¢ï¼š"{query}"
    åˆå§‹ç­”æ¡ˆï¼š{initial_answer}
    éªŒè¯ç»“æœæ‘˜è¦ï¼š{verification_summary}
    
    ä¿®æ­£åçš„æ–¹æ³•æŒ‡å—ï¼š
    """,
    
    "è§‚ç‚¹æŸ¥è¯¢": """
    ä½œä¸ºè§‚ç‚¹ç»¼è¿°ä¸“å®¶ï¼Œè¯·æ ¹æ®éªŒè¯ç»“æœé‡æ–°ç”Ÿæˆä¸€ä¸ªå¹³è¡¡å®¢è§‚çš„è§‚ç‚¹ç»¼è¿°ã€‚
    
    æŸ¥è¯¢æ„å›¾ï¼š{intent} - è§‚ç‚¹æŸ¥è¯¢
    åŸå§‹æŸ¥è¯¢ï¼š"{query}"
    åˆå§‹ç­”æ¡ˆï¼š{initial_answer}
    éªŒè¯ç»“æœæ‘˜è¦ï¼š{verification_summary}
    
    ä¿®æ­£åçš„è§‚ç‚¹ç»¼è¿°ï¼š
    """
}

# ==================== æ¯”è¾ƒåˆ†ææ¨¡æ¿ ====================
COMPARISON_ANALYSIS_TEMPLATE = """
# å›ç­”è´¨é‡æ¯”è¾ƒåˆ†ææŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯
- **åˆ†ææ—¶é—´**: {timestamp}
- **æŸ¥è¯¢ç±»å‹**: {intent}
- **åŸå§‹é—®é¢˜**: "{question}"

## å›ç­”å¯¹æ¯”

### åˆå§‹AIå›ç­”
{initial_answer}

**åˆå§‹å›ç­”ç‰¹ç‚¹**:
- ç”Ÿæˆé€Ÿåº¦: {initial_speed}
- è¯¦ç»†ç¨‹åº¦: {initial_detail}
- è‡ªä¿¡ç¨‹åº¦: {initial_confidence}

### éªŒè¯åå›ç­”
{verified_answer}

**éªŒè¯åå›ç­”ç‰¹ç‚¹**:
- å‡†ç¡®æ€§æå‡: {accuracy_improvement}
- è¯æ®æ”¯æŒåº¦: {evidence_support}
- å¯é æ€§è¯„çº§: {reliability_rating}

## å¹»è§‰æ£€æµ‹ç»“æœ
{hallucination_summary}

## å…³é”®æ”¹è¿›ç‚¹
{key_improvements}

## æ€»ä½“è¯„ä¼°
{overall_assessment}
"""

def get_initial_prompt(self, question: str) -> str:
    """è·å–åˆå§‹å›ç­”ç”Ÿæˆæç¤ºè¯"""
    return self.INITIAL_ANSWER_TEMPLATE.format(question=question)

def get_intent_classification_prompt(self, query: str) -> str:
    """è·å–æ„å›¾åˆ†ç±»æç¤ºè¯"""
    return self.INTENT_CLASSIFICATION_TEMPLATE.format(query=query)

def get_claim_extraction_prompt(self, text: str) -> str:
    """è·å–å£°æ˜æå–æç¤ºè¯"""
    return self.CLAIM_EXTRACTION_TEMPLATE.format(text=text)

def get_fact_verification_prompt(self, intent: str, query: str, claim: str, evidence_text: str) -> str:
    """è·å–äº‹å®éªŒè¯æç¤ºè¯"""
    return self.FACT_VERIFICATION_TEMPLATE.format(
        intent=intent,
        query=query,
        claim=claim,
        evidence_text=evidence_text
    )

def get_hallucination_detection_prompt(self, question: str, initial_answer: str, 
                                        verified_answer: str, evidence: str) -> str:
    """è·å–å¹»è§‰æ£€æµ‹æç¤ºè¯"""
    return self.HALLUCINATION_DETECTION_TEMPLATE.format(
        question=question,
        initial_answer=initial_answer,
        verified_answer=verified_answer,
        evidence=evidence
    )

def get_correction_prompt(self, intent: str, query: str, initial_answer: str, verification_summary: str) -> str:
    """è·å–ç­”æ¡ˆçº æ­£æç¤ºè¯"""
    template = self.CORRECTION_TEMPLATES.get(intent, self.CORRECTION_TEMPLATES["äº‹å®æŸ¥è¯¢"])
    return template.format(
        intent=intent,
        query=query,
        initial_answer=initial_answer,
        verification_summary=verification_summary
    )

def get_comparison_analysis_prompt(self, question: str, intent: str, initial_answer: str, 
                                verified_answer: str, hallucination_summary: str) -> str:
    """è·å–æ¯”è¾ƒåˆ†ææç¤ºè¯"""
    from datetime import datetime
    
    return self.COMPARISON_ANALYSIS_TEMPLATE.format(
        timestamp=datetime.now().isoformat(),
        intent=intent,
        question=question,
        initial_answer=initial_answer,
        verified_answer=verified_answer,
        initial_speed="å¿«é€Ÿ",
        initial_detail="è¯¦ç»†",
        initial_confidence="é«˜",
        accuracy_improvement="æ˜¾è‘—",
        evidence_support="å……åˆ†",
        reliability_rating="é«˜",
        hallucination_summary=hallucination_summary,
        key_improvements="1. äº‹å®å‡†ç¡®æ€§æå‡\n2. è¯æ®æ”¯æŒå¢å¼º\n3. é€»è¾‘ä¸€è‡´æ€§æ”¹å–„",
        overall_assessment="éªŒè¯è¿‡ç¨‹æ˜¾è‘—æå‡äº†å›ç­”çš„å¯é æ€§å’Œå‡†ç¡®æ€§"
    )


class EnhancedPipeline:
    """å¢å¼ºçš„æµç¨‹ç®¡ç†å™¨ - é›†æˆåˆå§‹å›ç­”ã€éªŒè¯å’Œå¹»è§‰æ£€æµ‹"""

def __init__(self, llm_client, templates):
    self.llm_client = llm_client
    self.templates = templates

def process_question(self, question: str) -> dict:
    """å¤„ç†é—®é¢˜çš„å®Œæ•´å¢å¼ºæµç¨‹"""
    
    # 1. ç”Ÿæˆåˆå§‹å›ç­”
    print("ğŸ”„ ç”Ÿæˆåˆå§‹AIå›ç­”...")
    initial_prompt = self.templates.get_initial_answer_prompt(question)
    initial_answer = self.llm_client.generate_response(initial_prompt)
    
    # 2. æ„å›¾åˆ†ç±»
    print("ğŸ¯ åˆ†ææŸ¥è¯¢æ„å›¾...")
    intent_prompt = self.templates.get_intent_classification_prompt(question)
    intent = self.llm_client.generate_response(intent_prompt)
    
    # 3. å£°æ˜æå–
    print("ğŸ” æå–å›ç­”ä¸­çš„å£°æ˜...")
    claim_prompt = self.templates.get_claim_extraction_prompt(initial_answer)
    claims_text = self.llm_client.generate_response(claim_prompt)
    
    # 4. äº‹å®éªŒè¯ï¼ˆæ¨¡æ‹Ÿè¯æ®ï¼‰
    print("âœ… è¿›è¡Œäº‹å®éªŒè¯...")
    evidence = "ç›¸å…³è¯æ®å†…å®¹..."  # è¿™é‡Œåº”è¯¥æ˜¯å®é™…çš„æ£€ç´¢ç»“æœ
    verification_results = []
    
    # 5. ç”ŸæˆéªŒè¯åå›ç­”
    print("âœï¸ ç”ŸæˆéªŒè¯åå›ç­”...")
    verification_summary = "éªŒè¯ç»“æœæ‘˜è¦..."
    correction_prompt = self.templates.get_correction_prompt(
        intent, question, initial_answer, verification_summary
    )
    verified_answer = self.llm_client.generate_response(correction_prompt)
    
    # 6. å¹»è§‰æ£€æµ‹
    print("ğŸ”¬ è¿›è¡Œå¹»è§‰æ£€æµ‹...")
    hallucination_prompt = self.templates.get_hallucination_detection_prompt(
        question, initial_answer, verified_answer, evidence
    )
    hallucination_analysis = self.llm_client.generate_response(hallucination_prompt)
    
    # 7. æ¯”è¾ƒåˆ†æ
    print="ğŸ“Š ç”Ÿæˆæ¯”è¾ƒåˆ†ææŠ¥å‘Š..."
    comparison_prompt = self.templates.get_comparison_analysis_prompt(
        question, intent, initial_answer, verified_answer, hallucination_analysis
    )
    comparison_report = self.llm_client.generate_response(comparison_prompt)
    
    return {
        "question": question,
        "intent": intent,
        "initial_answer": initial_answer,
        "verified_answer": verified_answer,
        "verification_results": verification_results,
        "hallucination_analysis": hallucination_analysis,
        "comparison_report": comparison_report,
        "processing_metadata": {
            "timestamp": self._get_timestamp(),
            "steps_completed": [
                "initial_answer_generation",
                "intent_classification", 
                "claim_extraction",
                "fact_verification",
                "answer_correction",
                "hallucination_detection",
                "comparison_analysis"
            ]
        }
    }

def _get_timestamp(self):
    """è·å–æ—¶é—´æˆ³"""
    from datetime import datetime
    return datetime.now().isoformat()


# # ==================== ä½¿ç”¨ç¤ºä¾‹å’Œæµ‹è¯• ====================
# def demonstrate_enhanced_pipeline():
#     """æ¼”ç¤ºå¢å¼ºç‰ˆæµç¨‹"""
    
#     # æ¨¡æ‹ŸLLMå®¢æˆ·ç«¯
#     class MockLLMClient:
#         def generate_response(self, prompt):
#             return f"æ¨¡æ‹Ÿå“åº”: {prompt[:50]}..."
    
#     # åˆå§‹åŒ–ç»„ä»¶
#     templates = PromptTemplates()
#     llm_client = MockLLMClient()
#     pipeline = EnhancedPipeline(llm_client, templates)
    
#     # æµ‹è¯•é—®é¢˜
#     test_question = "äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•è¶‹åŠ¿æ˜¯ä»€ä¹ˆï¼Ÿ"
    
#     print("ğŸš€ å¼€å§‹å¢å¼ºç‰ˆæµç¨‹æ¼”ç¤º")
#     print("=" * 60)
    
#     # æ‰§è¡Œå®Œæ•´æµç¨‹
#     result = pipeline.process_question(test_question)
    
#     # æ˜¾ç¤ºç»“æœ
#     print("\nğŸ“‹ å¤„ç†ç»“æœæ‘˜è¦:")
#     print(f"é—®é¢˜: {result['question']}")
#     print(f"æ£€æµ‹åˆ°çš„æ„å›¾: {result['intent']}")
#     print(f"åˆå§‹å›ç­”é•¿åº¦: {len(result['initial_answer'])} å­—ç¬¦")
#     print(f"éªŒè¯åå›ç­”é•¿åº¦: {len(result['verified_answer'])} å­—ç¬¦")
#     print(f"æ˜¯å¦æ£€æµ‹åˆ°å¹»è§‰: {'æ˜¯' if 'hallucination' in str(result['hallucination_analysis']) else 'å¦'}")
    
#     print("\nğŸ“Š æ¯”è¾ƒåˆ†æ:")
#     print(result['comparison_report'][:200] + "...")
    
#     return result


# def test_template_functionality():
#     """æµ‹è¯•æ¨¡æ¿åŠŸèƒ½å®Œæ•´æ€§"""
    
#     templates = PromptTemplates()
    
#     # æµ‹è¯•æ‰€æœ‰æ¨¡æ¿æ–¹æ³•
#     test_cases = [
#         {
#             "name": "åˆå§‹å›ç­”ç”Ÿæˆ",
#             "method": templates.get_initial_answer_prompt,
#             "args": ["æµ‹è¯•é—®é¢˜"]
#         },
#         {
#             "name": "æ„å›¾åˆ†ç±»", 
#             "method": templates.get_intent_classification_prompt,
#             "args": ["æµ‹è¯•æŸ¥è¯¢"]
#         },
#         {
#             "name": "å¹»è§‰æ£€æµ‹",
#             "method": templates.get_hallucination_detection_prompt,
#             "args": ["é—®é¢˜", "åˆå§‹å›ç­”", "éªŒè¯å›ç­”", "è¯æ®"]
#         }
#     ]
    
#     print("ğŸ§ª æ¨¡æ¿åŠŸèƒ½æµ‹è¯•")
#     print("=" * 40)
    
#     for test_case in test_cases:
#         try:
#             result = test_case["method"](*test_case["args"])
#             print(f"âœ… {test_case['name']}: æˆåŠŸç”Ÿæˆæç¤ºè¯")
#             print(f"   æ ·ä¾‹: {result[:80]}...")
#         except Exception as e:
#             print(f"âŒ {test_case['name']}: å¤±è´¥ - {e}")


# if __name__ == "__main__":
#     # è¿è¡ŒåŠŸèƒ½æµ‹è¯•
#     test_template_functionality()
    
#     print("\n" + "="*60)
    
#     # æ¼”ç¤ºå¢å¼ºæµç¨‹
#     demonstrate_enhanced_pipeline()
# """
# # åˆå§‹åŒ–æµç¨‹
# from src.llm.deepseek_client import DeepSeekClient
# from src.llm.prompt_templates import PromptTemplates, EnhancedPipeline

# # åˆ›å»ºç»„ä»¶
# llm_client = DeepSeekClient(config)
# templates = PromptTemplates()
# pipeline = EnhancedPipeline(llm_client, templates)

# # æ‰§è¡Œå®Œæ•´æµç¨‹
# question = "é‡å­è®¡ç®—å¯¹å¯†ç å­¦çš„å½±å“æ˜¯ä»€ä¹ˆï¼Ÿ"
# result = pipeline.process_question(question)

# # åˆ†æç»“æœ
# print("åˆå§‹å›ç­”:", result['initial_answer'])
# print("éªŒè¯åå›ç­”:", result['verified_answer'])
# print("å¹»è§‰åˆ†æ:", result['hallucination_analysis'])
# print("æ¯”è¾ƒæŠ¥å‘Š:", result['comparison_report'])
# """