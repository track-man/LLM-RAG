"""
éªŒè¯æ¨¡å—æµ‹è¯•ç”¨ä¾‹
æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨fact_checkerè¿›è¡Œå›ç­”éªŒè¯
"""
import sys
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.verification.fact_checker import verify_answer, VerificationLevel
import config

def test_basic_verification():
    """æµ‹è¯•åŸºç¡€éªŒè¯åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•åŸºç¡€éªŒè¯åŠŸèƒ½")
    print("=" * 50)
    
    # æ¨¡æ‹Ÿæ£€ç´¢åˆ°çš„æ–‡æ¡£å—
    retrieved_chunks = [
        {
            "text": "BAAI/bge-base-en-v1.5æ˜¯ä¸€ä¸ªç”±åŒ—äº¬äººå·¥æ™ºèƒ½ç ”ç©¶é™¢å¼€å‘çš„åµŒå…¥æ¨¡å‹ï¼Œè¾“å‡ºå‘é‡ç»´åº¦ä¸º768ç»´ã€‚",
            "metadata": {"source": "embedding_model_info.txt"},
            "distance": 0.1
        },
        {
            "text": "è¯¥æ¨¡å‹åœ¨å¤šä¸ªè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­è¡¨ç°ä¼˜å¼‚ï¼ŒåŒ…æ‹¬æ–‡æœ¬ç›¸ä¼¼åº¦è®¡ç®—å’Œæ–‡æ¡£æ£€ç´¢ã€‚",
            "metadata": {"source": "model_performance.txt"},
            "distance": 0.2
        }
    ]
    
    # æµ‹è¯•æ¡ˆä¾‹1ï¼šæ­£ç¡®å›ç­”
    answer1 = "BAAI/bge-base-en-v1.5åµŒå…¥æ¨¡å‹çš„è¾“å‡ºå‘é‡ç»´åº¦æ˜¯768ç»´ã€‚"
    result1 = verify_answer(answer1, retrieved_chunks, "åµŒå…¥æ¨¡å‹ç»´åº¦", "basic")
    
    print(f"æµ‹è¯•æ¡ˆä¾‹1 - æ­£ç¡®å›ç­”:")
    print(f"å›ç­”: {answer1}")
    print(f"å¹»è§‰æ£€æµ‹: {'æ˜¯' if result1.has_hallucination else 'å¦'}")
    print(f"ç½®ä¿¡åº¦: {result1.confidence_score:.3f}")
    print(f"é”™è¯¯æè¿°: {result1.error_descriptions}")
    print()
    
    # æµ‹è¯•æ¡ˆä¾‹2ï¼šåŒ…å«å¹»è§‰çš„å›ç­”
    answer2 = "BAAI/bge-base-en-v1.5åµŒå…¥æ¨¡å‹çš„è¾“å‡ºå‘é‡ç»´åº¦æ˜¯1024ç»´ï¼Œå¹¶ä¸”è¯¥æ¨¡å‹å¯ä»¥å¤„ç†å›¾åƒè¾“å…¥ã€‚"
    result2 = verify_answer(answer2, retrieved_chunks, "åµŒå…¥æ¨¡å‹ç»´åº¦", "basic")
    
    print(f"æµ‹è¯•æ¡ˆä¾‹2 - åŒ…å«å¹»è§‰çš„å›ç­”:")
    print(f"å›ç­”: {answer2}")
    print(f"å¹»è§‰æ£€æµ‹: {'æ˜¯' if result2.has_hallucination else 'å¦'}")
    print(f"ç½®ä¿¡åº¦: {result2.confidence_score:.3f}")
    print(f"é”™è¯¯æè¿°: {result2.error_descriptions}")
    print()

def test_comprehensive_verification():
    """æµ‹è¯•ç»¼åˆéªŒè¯åŠŸèƒ½"""
    print("ğŸ§ª æµ‹è¯•ç»¼åˆéªŒè¯åŠŸèƒ½")
    print("=" * 50)
    
    # æ¨¡æ‹Ÿæ£€ç´¢åˆ°çš„æ–‡æ¡£å—
    retrieved_chunks = [
        {
            "text": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œå®ƒä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„è¡¨ç¤ºã€‚",
            "metadata": {"source": "deep_learning_intro.txt"},
            "distance": 0.1
        },
        {
            "text": "å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ç‰¹åˆ«é€‚ç”¨äºå›¾åƒå¤„ç†ä»»åŠ¡ï¼Œè€Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰é€‚ç”¨äºåºåˆ—æ•°æ®ã€‚",
            "metadata": {"source": "neural_networks.txt"},
            "distance": 0.15
        }
    ]
    
    # æµ‹è¯•æ¡ˆä¾‹ï¼šéƒ¨åˆ†æ­£ç¡®çš„å›ç­”
    answer = "æ·±åº¦å­¦ä¹ ä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œï¼ŒCNNä¸»è¦ç”¨äºå›¾åƒå¤„ç†ï¼Œä½†RNNä¹Ÿå¯ä»¥å¤„ç†å›¾åƒä»»åŠ¡ã€‚"
    result = verify_answer(answer, retrieved_chunks, "æ·±åº¦å­¦ä¹ ç½‘ç»œç±»å‹", "comprehensive")
    
    print(f"æµ‹è¯•æ¡ˆä¾‹ - éƒ¨åˆ†æ­£ç¡®çš„å›ç­”:")
    print(f"å›ç­”: {answer}")
    print(f"å¹»è§‰æ£€æµ‹: {'æ˜¯' if result.has_hallucination else 'å¦'}")
    print(f"ç½®ä¿¡åº¦: {result.confidence_score:.3f}")
    print(f"éªŒè¯çº§åˆ«: {result.verification_level}")
    print(f"é”™è¯¯æè¿°: {result.error_descriptions}")
    print(f"æ”¯æŒè¯æ®æ•°é‡: {len(result.evidence_chunks)}")
    print()

def test_verification_levels():
    """æµ‹è¯•ä¸åŒéªŒè¯çº§åˆ«"""
    print("ğŸ§ª æµ‹è¯•ä¸åŒéªŒè¯çº§åˆ«")
    print("=" * 50)
    
    retrieved_chunks = [
        {
            "text": "Pythonæ˜¯ä¸€ç§è§£é‡Šå‹ã€é¢å‘å¯¹è±¡ã€åŠ¨æ€æ•°æ®ç±»å‹çš„é«˜çº§ç¨‹åºè®¾è®¡è¯­è¨€ã€‚",
            "metadata": {"source": "python_info.txt"},
            "distance": 0.1
        }
    ]
    
    answer = "Pythonæ˜¯ä¸€ç§ç¼–è¯‘å‹ç¼–ç¨‹è¯­è¨€ï¼Œä¸»è¦ç”¨äºWebå¼€å‘ã€‚"
    
    # æµ‹è¯•ä¸åŒéªŒè¯çº§åˆ«
    levels = ["basic", "semantic", "comprehensive"]
    
    for level in levels:
        result = verify_answer(answer, retrieved_chunks, "Pythonè¯­è¨€ç‰¹ç‚¹", level)
        print(f"éªŒè¯çº§åˆ«: {level}")
        print(f"  å¹»è§‰æ£€æµ‹: {'æ˜¯' if result.has_hallucination else 'å¦'}")
        print(f"  ç½®ä¿¡åº¦: {result.confidence_score:.3f}")
        print(f"  é”™è¯¯æ•°é‡: {len(result.error_descriptions)}")
        print()

def create_sample_documents():
    """åˆ›å»ºç¤ºä¾‹æ–‡æ¡£ç”¨äºæµ‹è¯•"""
    print("ğŸ“ åˆ›å»ºç¤ºä¾‹æ–‡æ¡£")
    print("=" * 50)
    
    # åˆ›å»ºraw_docsç›®å½•
    os.makedirs(config.RAW_DOC_DIR, exist_ok=True)
    
    # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£
    sample_docs = {
        "embedding_models.txt": """
BAAI/bge-base-en-v1.5æ˜¯ç”±åŒ—äº¬äººå·¥æ™ºèƒ½ç ”ç©¶é™¢å¼€å‘çš„åµŒå…¥æ¨¡å‹ã€‚
è¯¥æ¨¡å‹åœ¨MTEBåŸºå‡†æµ‹è¯•ä¸­è¡¨ç°ä¼˜å¼‚ï¼Œç‰¹åˆ«æ˜¯åœ¨æ£€ç´¢ä»»åŠ¡ä¸Šã€‚
æ¨¡å‹è¾“å‡ºå‘é‡ç»´åº¦ä¸º768ç»´ï¼Œæ”¯æŒæœ€å¤§åºåˆ—é•¿åº¦512ã€‚
""",
        
        "deep_learning.txt": """
æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯ã€‚
å®ƒä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥å­¦ä¹ æ•°æ®çš„å¤æ‚è¡¨ç¤ºã€‚
å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ç‰¹åˆ«é€‚ç”¨äºå›¾åƒå¤„ç†ä»»åŠ¡ã€‚
å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰é€‚ç”¨äºå¤„ç†åºåˆ—æ•°æ®ï¼Œå¦‚æ–‡æœ¬å’Œæ—¶é—´åºåˆ—ã€‚
Transformeræ¶æ„åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­å–å¾—äº†çªç ´æ€§è¿›å±•ã€‚
""",
        
        "python_language.txt": """
Pythonæ˜¯ä¸€ç§è§£é‡Šå‹ã€é¢å‘å¯¹è±¡ã€åŠ¨æ€æ•°æ®ç±»å‹çš„é«˜çº§ç¨‹åºè®¾è®¡è¯­è¨€ã€‚
Pythonè¯­è¨€å…·æœ‰ç®€æ´çš„è¯­æ³•å’Œå¼ºå¤§çš„åŠŸèƒ½ã€‚
Pythonå¹¿æ³›åº”ç”¨äºWebå¼€å‘ã€æ•°æ®ç§‘å­¦ã€äººå·¥æ™ºèƒ½ç­‰é¢†åŸŸã€‚
Pythonæ‹¥æœ‰ä¸°å¯Œçš„ç¬¬ä¸‰æ–¹åº“å’Œæ¡†æ¶ã€‚
"""
    }
    
    for filename, content in sample_docs.items():
        filepath = os.path.join(config.RAW_DOC_DIR, filename)
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"åˆ›å»ºæ–‡æ¡£: {filepath}")
    
    print(f"ç¤ºä¾‹æ–‡æ¡£åˆ›å»ºå®Œæˆï¼Œå…±{len(sample_docs)}ä¸ªæ–‡æ¡£")
    print()

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ” éªŒè¯æ¨¡å—æµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºç¤ºä¾‹æ–‡æ¡£
    create_sample_documents()
    
    # è¿è¡Œæµ‹è¯•
    test_basic_verification()
    test_comprehensive_verification()
    test_verification_levels()
    
    print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆ")

if __name__ == "__main__":
    main()